services:
  tabbyapi-base:
    # Uncomment this to build a docker image from source
    # build:
    #   context: .
    #   dockerfile: ./Dockerfile
    #   target: tabbyapi-exllamav2-builder-image-runtime

    # Comment this to build a docker image from source
    image: gitlab-registry.rancher.devguy.dev/davidallada/llm-gpu-clone
    runtime: nvidia
    privileged: true # Required for mounting NFS
    deploy:
      resources:
        reservations:
          devices:
          - driver: "nvidia"
            capabilities: [ "gpu" ]
            count: all
    volumes:
      - pip-cache:/cache/pip
      - apt-cache:/cache/apt
      - /mnt/docker_data/llm-gpu-clone/tabby_data:/app/tabby_data
      - /mnt/docker_data/llm-gpu-clone/model_directories:/app/model_directories
      # - /mnt/docker_data/llm-gpu-clone/tabby_logs:/var/log/tabbyapi/
    command: exit

  tabbyapi:
    extends:
      service: tabbyapi-base
    container_name: tabbyapi-webserver
    restart: always
    command: webserver
    environment:
      - NAME=TabbyAPI
      - CUDA_VISIBLE_DEVICES=GPU-2652c5d6-641a-fa93-418c-3e9afd215a34,GPU-2f4ddefb-b5ee-8687-eeca-da77f315b228,GPU-da6c157e-0c5f-0968-a088-185d90537446
      - GGUF_STORAGE_DIR=/mnt/exllamav2/gguf
      - EXL2_STORAGE_DIR=/mnt/exllamav2/exl2
      - MEASUREMENT_JSON_DIR=/mnt/exllamav2/measurement_json
      - MODEL_LOAD_FILE=/app/tabby_data/tabby_model_load_config.json
    ports:
      - "5000:5000"
      - "8076:8888"
    volumes:
      - /mnt/docker_data/llm-gpu-clone/tabby_data:/app/tabby_data
      - /mnt/docker_data/llm-gpu-clone/model_directories/primary:/app/model_directories
      - /mnt/docker_data/llm-gpu-clone/tabby_logs/primary:/var/log/tabbyapi
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  tabbyapi-converter:
    restart: always
    extends:
      service: tabbyapi-base
    ports:
      - "5001:5000"
      - "8077:8888"
    command: converter
    environment:
      - NAME=TabbyAPI
      - CUDA_VISIBLE_DEVICES=GPU-2652c5d6-641a-fa93-418c-3e9afd215a34,GPU-2f4ddefb-b5ee-8687-eeca-da77f315b228,GPU-da6c157e-0c5f-0968-a088-185d90537446
      - GGUF_STORAGE_DIR=/mnt/exllamav2/gguf
      - EXL2_STORAGE_DIR=/mnt/exllamav2/exl2
      - MEASUREMENT_JSON_DIR=/mnt/exllamav2/measurement_json

  tabbyapi-embeddings:
    restart: always
    extends:
      service: tabbyapi-base
    ports:
      - "5002:5000"
      - "8078:8888"
    command: embeddings_only
    environment:
      - NAME=TabbyAPI
      - CUDA_VISIBLE_DEVICES=GPU-da6c157e-0c5f-0968-a088-185d90537446
      - GGUF_STORAGE_DIR=/mnt/exllamav2/gguf
      - EXL2_STORAGE_DIR=/mnt/exllamav2/exl2
      - MEASUREMENT_JSON_DIR=/mnt/exllamav2/measurement_json
      - MODEL_LOAD_FILE=/app/tabby_data/alt_tabby_model_load_config.json
    volumes:
      - /mnt/docker_data/llm-gpu-clone/tabby_data:/app/tabby_data
      - /mnt/docker_data/llm-gpu-clone/model_directories/secondary:/app/model_directories
      - /mnt/docker_data/llm-gpu-clone/tabby_logs/secondary:/var/log/tabbyapi
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  tabbyapi-dev:
    restart: always
    extends:
      service: tabbyapi-base
    ports:
      - "5003:5000"
      - "8079:8888"
    command: embeddings_only
    environment:
      - TABBY_LOG_LEVEL=DEBUG
      - NAME=TabbyAPI
      - CUDA_VISIBLE_DEVICES=GPU-da6c157e-0c5f-0968-a088-185d90537446
      - GGUF_STORAGE_DIR=/mnt/exllamav2/gguf
      - EXL2_STORAGE_DIR=/mnt/exllamav2/exl2
      - MEASUREMENT_JSON_DIR=/mnt/exllamav2/measurement_json
      - MODEL_LOAD_FILE=/app/tabby_data/alt_tabby_model_load_config.json
    volumes:
      - /mnt/docker_data/llm-gpu-clone/tabby_data:/app/tabby_data
      - /mnt/docker_data/llm-gpu-clone/model_directories/secondary:/app/model_directories
      - /home/serveradmin/tabbyAPI:/app/tabbyAPI
      - /mnt/docker_data/llm-gpu-clone/tabby_logs/dev:/var/log/tabbyapi
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3


# curl http://10.69.1.169:5000/v1/model/load \
#   -H "Content-Type: application/json" \
#   -H "x-api-key: <api_key>" \
#   -H "x-admin-key: <api_key>" \
#   -H "Authorization: Bearer <api_key>" \
#   -d '{
#     "model_name": "Qwen_Qwen2.5-Coder-32B-Instruct-4.0",
#     "draft_model": {
#       "draft_model_name": "Qwen_Qwen2.5-Coder-0.5B-Instruct-4.0-bpw"
#     }
#   }'


volumes:
  pip-cache:
  apt-cache:
