{
    "model_configs": [
        {
            "pretty_model_name": "Qwen 2.5 14B 400k",
            "model_name": "Qwen_Qwen2.5-14B-Instruct-1M-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 253952
            },
            "comment": "1m 14b"
        },
        {
            "pretty_model_name": "Qwen 2.5 7B 400k",
            "model_name": "Qwen_Qwen2.5-7B-Instruct-1M-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 409600
            },
            "comment": "1m 14b"
        },
        {
            "pretty_model_name": "Qwen 2.5 7B 1M",
            "model_name": "Qwen_Qwen2.5-7B-Instruct-1M-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 999424
            },
            "comment": "1m 14b"
        },
        {
            "pretty_model_name": "Qwen2.5-Coder-32B-daily-6.5bpw-100k",
            "model_name": "Qwen_Qwen2.5-Coder-32B-Instruct-6.5-bpw",
            "draft_model": "Qwen_Qwen2.5-Coder-1.5B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 102400
            },
            "comment": "Coding Daily Driver, Full 48GB of VRAM"
        },
        {
            "pretty_model_name": "Qwen2.5-Coder-32B-single-GPU-12k",
            "model_name": "Qwen_Qwen2.5-Coder-32B-Instruct-4.0-bpw",
            "draft_model": "Qwen_Qwen2.5-Coder-1.5B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 12288
            },
            "comment": "Fits in one GPU with decent context"
        },
        {
            "pretty_model_name": "Qwen2.5-72B-16k",
            "model_name": "Qwen_Qwen2.5-72B-Instruct-4.0-bpw",
            "draft_model": "Qwen_Qwen2.5-Coder-1.5B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 16384
            },
            "comment": "Qwen 2.5 72B fits in two GPU's with 12k context"
        },

        {
            "pretty_model_name": "Qwen_QVQ-72B-preview-4.0-bpw",
            "model_name": "Qwen_QVQ-72B-preview-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 16384
            },
            "comment": "QVQ"
        },
        {
            "pretty_model_name": "QwQ-Preview-32B-draft",
            "model_name": "Qwen_QwQ-32B-preview-6.0-bpw",
            "draft_model": "PowerInfer_SmallThinker-3B-Preview-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "QwQ Preview"
        },
        {
            "pretty_model_name": "homeassistant-llama-8b-4bpw-8k",
            "model_name": "meta-llama_Llama-3.1-8B-Instruct-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 8192
            },
            "comment": "For homeassistant"
        },
        {
            "pretty_model_name": "llama-8B-4bpw-fast-32k",
            "model_name": "meta-llama_Llama-3.1-8B-Instruct-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "Super fast 150 tokens/sec"
        },
        {
            "pretty_model_name": "llama-8B-8bpw-32k",
            "model_name": "meta-llama_Llama-3.1-8B-Instruct-8.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "Higher quality llama 8B with 32k context"
        },
        {
            "pretty_model_name": "llama-8B-4bpw-fast-128k",
            "model_name": "meta-llama_Llama-3.1-8B-Instruct-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 128000
            },
            "comment": "Super long context"
        },
        {
            "pretty_model_name": "llama-8B-8bpw-128k",
            "model_name": "meta-llama_Llama-3.1-8B-Instruct-8.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 128000
            },
            "comment": "Super long context"
        },
        {
            "pretty_model_name": "Llama-3.3-70B-draft",
            "model_name": "meta-llama_Llama-3.3-70B-Instruct-4.0-bpw",
            "draft_model": "meta-llama_Llama-3.2-1B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 12288
            },
            "comment": "Big LLama"
        },
        {
            "pretty_model_name": "Nemotron-70B-draft",
            "model_name": "nvidia_Llama-3.1-Nemotron-70B-Instruct-HF-4.0-bpw",
            "draft_model": "meta-llama_Llama-3.2-1B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 12288
            },
            "comment": "Big LLama"
        },
        {
            "pretty_model_name": "internlm3-8b-4.0bpw-32k",
            "model_name": "internlm_internlm3-8b-instruct-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "Intern LM"
        },
        {
            "pretty_model_name": "internlm3-8b-8.0bpw-32k",
            "model_name": "internlm_internlm3-8b-instruct-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "Intern LM"
        },
        {
            "pretty_model_name": "Nemotron-70B-draft-4.5bpw",
            "model_name": "nvidia_Llama-3.1-Nemotron-70B-Instruct-HF-4.5-bpw",
            "draft_model": "meta-llama_Llama-3.2-1B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 12288
            },
            "comment": "Big nemotron 4.5"
        },
        {
            "pretty_model_name": "R1-Distill-Llama-70B-4bpw",
            "model_name": "deepseek-ai_DeepSeek-R1-Distill-Llama-70B-4.0-bpw",
            "draft_model": "",
            "tabby_config": {
                "max_seq_len": 8192
            },
            "comment": "Big deepseek"
        },
        {
            "pretty_model_name": "R1-Distill-Llama-70B-4bpw-draft",
            "model_name": "deepseek-ai_DeepSeek-R1-Distill-Llama-70B-4.0-bpw",
            "draft_model": "deepseek-ai_DeepSeek-R1-Distill-Llama-8B-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 8192
            },
            "comment": "Big deepseek"
        },
        {
            "pretty_model_name": "R1-Distill-Qwen2.5-6.5bpw-draft",
            "model_name": "deepseek-ai_DeepSeek-R1-Distill-Qwen-32B-6.5-bpw",
            "draft_model": "deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 32768
            },
            "comment": "Big deepseek"
        },
        {
            "pretty_model_name": "Qwen2.5-VL-72B-Draft",
            "model_name": "Qwen_Qwen2.5-VL-72B-Instruct-4.0-bpw",
            "draft_model": "Qwen_Qwen2.5-VL-3B-Instruct-4.0-bpw",
            "tabby_config": {
                "max_seq_len": 8192
            },
            "comment": "Big deepseek"
        }
    ],
    "embedding_configs": [
        {
            "pretty_model_name": "dunzhang_stella_en_1.5B_v5",
            "model_name": "dunzhang_stella_en_1.5B_v5",
            "comment": "Stella"
        },
        {
            "pretty_model_name": "google_Gemma-Embeddings-v1.0",
            "model_name": "google_Gemma-Embeddings-v1.0",
            "comment": "Gemma"
        },
        {
            "pretty_model_name": "answerdotai_ModernBERT-large",
            "model_name": "answerdotai_ModernBERT-large",
            "comment": "ModernBERT"
        }
    ],
    "lora_configs": []
}
